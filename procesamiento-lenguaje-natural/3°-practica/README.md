

- Normalizaci√≥n de espacios en blanco.  
- Tokenizaci√≥n en palabras.

> Para esta etapa se utiliz√≥ la funci√≥n `preprocess_text` desarrollada previamente en pr√°cticas anteriores.

---

### 2. Conjuntos de datos analizados
Se utilizaron tres archivos CSV con t√≠tulos de noticias, trabajando de manera independiente en cada uno:

- `noticias_proceso.csv`
- `noticias_elpais.csv`
- `noticias_elfinanciero.csv`

De cada archivo se ley√≥ la columna **`title`**, que fue preprocesada y unida en un solo texto.

---

### 3. C√°lculo de asociaciones con PMI
1. A partir del texto preprocesado se generaron **bigramas** (pares de palabras consecutivas).  
2. Se calcularon las probabilidades:
 - Unigramas: `p(x)`  
 - Bigramas: `p(x, y)`  
3. Se aplic√≥ la f√≥rmula de PMI:  

 \[
 PMI(x,y) = \log_2 \frac{p(x,y)}{p(x)\,p(y)}
 \]

4. Se filtraron asociaciones que aparecen **al menos 10 veces** en el texto.  
5. Se seleccionaron las **10 asociaciones m√°s significativas** con mayor valor de PMI.

---

### 4. Resultados esperados
Para cada dataset (`Proceso`, `El Pa√≠s`, `El Financiero`) se obtuvo una tabla con las siguientes columnas:

- **w1** ‚Üí Primera palabra del bigrama.  
- **w2** ‚Üí Segunda palabra del bigrama.  
- **freq** ‚Üí Frecuencia absoluta de aparici√≥n del bigrama.  
- **pmi** ‚Üí Valor de informaci√≥n mutua puntual.  

La tabla presenta los **10 bigramas con mayor PMI**, es decir, las asociaciones de palabras m√°s relevantes.

---

## üìí Notebook
Todo el procedimiento se implement√≥ en un **notebook de Jupyter** que:
- Lee los datasets.  
- Preprocesa los textos.  
- Genera tokens y bigramas.  
- Calcula probabilidades y PMI.  
- Muestra las 10 asociaciones m√°s significativas por dataset.  

---

##  Conclusi√≥n
La pr√°ctica permiti√≥:
- Aplicar t√©cnicas de preprocesamiento de texto.  
- Construir modelos simples de lenguaje basados en n-gramas.  
- Identificar asociaciones relevantes en lenguaje natural mediante **PMI**.  

